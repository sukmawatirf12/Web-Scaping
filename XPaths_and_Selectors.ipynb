{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugCCH5PaB84g"
      },
      "source": [
        "# XPaths and Selectors\n",
        "\n",
        "> Leverage XPath syntax to explore scrapy selectors. Both of these concepts will move you towards being able to scrape an HTML document. This is the Summary of lecture \"Web Scraping in Python\", via datacamp.\n",
        "\n",
        "- toc: true \n",
        "- badges: true\n",
        "- comments: true\n",
        "- author: Chanseok Kang\n",
        "- categories: [Python, Datacamp]\n",
        "- image: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpF7h6bZPc44",
        "outputId": "50eb61aa-81f0-4cda-befe-faedc4f2d161"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.9.0-py2.py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.2/277.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted>=18.9.0 (from scrapy)\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.4.6 in /usr/local/lib/python3.10/dist-packages (from scrapy) (40.0.2)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Collecting pyOpenSSL>=21.0.0 (from scrapy)\n",
            "  Downloading pyOpenSSL-23.2.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.2)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.4.6->scrapy) (1.15.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
            "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.27.1)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy) (2.21)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.12)\n",
            "Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy\n",
            "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.8.1 protego-0.2.1 pyOpenSSL-23.2.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.9.0 service-identity-21.1.0 tldextract-3.4.4 w3lib-2.1.1 zope.interface-6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KJ6qIHBnB84l"
      },
      "outputs": [],
      "source": [
        "from scrapy import Selector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K9PvRynB84n"
      },
      "source": [
        "## XPathology\n",
        "- Slashes and Brackets\n",
        "    - Single forward slash `/` looks forward one generation\n",
        "    - Double forward slash `//` looks forward all future generations\n",
        "    - Square brackets `[]` help narrow in on specific elements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPDrgtxwB84n"
      },
      "source": [
        "## Off the Beaten XPath\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M67p3HKXB84o"
      },
      "source": [
        "## Selector Objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evvCN7_jB84o"
      },
      "source": [
        "### XPath Chaining\n",
        "`Selector` and `SelectorList` objects allow for chaining when using the `xpath` method. What this means is that you can apply the `xpath` method over once you've already applied it. For example, if sel is the name of our Selector, then\n",
        "```\n",
        "sel.xpath('/html/body/div[2]')\n",
        "```\n",
        "is the same as\n",
        "```\n",
        "sel.xpath('/html').xpath('./body/div[2]')\n",
        "```\n",
        "or is the same as\n",
        "```\n",
        "sel.xpath('/html').xpath('./body').xpath('./div[2]')\n",
        "```\n",
        "The only catch is that you need to \"glue together\" the XPath pieces by using a period at the start of each subsequent XPath string (notice the periods we added to the XPath strings in our examples)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F5KMIIShB84p"
      },
      "outputs": [],
      "source": [
        "html = '''\n",
        "<html>\n",
        "    <body>\n",
        "        <div>HELLO</div>\n",
        "        <div>\n",
        "            <p>GOODBYE</p>\n",
        "        </div>\n",
        "        <div>\n",
        "            <span>\n",
        "                <p>NOPE</p>\n",
        "                <p>ALMOST</p>\n",
        "                <p>YOU GOT IT!</p>\n",
        "            </span>\n",
        "        </div>\n",
        "    </body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "sel = Selector(text=html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TdwEzcpB84p",
        "outputId": "d3d5eeb0-6439-464b-aada-e42c2778e6e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Selector query='./span/p[3]' data='<p>YOU GOT IT!</p>'>]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Chain together xpath methods to select desired p element\n",
        "sel.xpath('//div').xpath('./span/p[3]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dazsggXsB84q"
      },
      "source": [
        "### Divvy Up This Exercise\n",
        "You will use this `html` variable as the HTML document to set up a `Selector` object with, and create a `SelectorList` which selects all `div` elements; then, you will check your understanding of what happens within the `SelectorList`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gfQ8nD-JB84r"
      },
      "outputs": [],
      "source": [
        "html = '''\n",
        "<html>\n",
        "    <body>\n",
        "        <div>Div 1: <p>paragraph 1</p></div>\n",
        "        <div>Div 2: <p>paragraph 2</p> <p>paragraph 3</p> </div>\n",
        "        <div>Div 3: <p>paragraph 4</p> <p>paragraph 5</p> <p>paragraph 6</p></div>\n",
        "        <div>Div 4: <p>paragraph 7</p></div>\n",
        "        <div>Div 5: <p>paragraph 8</p></div>\n",
        "    </body>\n",
        "</html>\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ayn2XIbnB84r",
        "outputId": "a54092eb-71da-492f-f51d-0d3da4494d91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Selector query='//div' data='<div>Div 1: <p>paragraph 1</p></div>'>,\n",
              " <Selector query='//div' data='<div>Div 2: <p>paragraph 2</p> <p>par...'>,\n",
              " <Selector query='//div' data='<div>Div 3: <p>paragraph 4</p> <p>par...'>,\n",
              " <Selector query='//div' data='<div>Div 4: <p>paragraph 7</p></div>'>,\n",
              " <Selector query='//div' data='<div>Div 5: <p>paragraph 8</p></div>'>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Create a Selector selecting html as the HTML document\n",
        "sel = Selector(text=html)\n",
        "\n",
        "# Create a SelectorList of all div elements in the HTML document\n",
        "divs = sel.xpath('//div')\n",
        "divs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5efFf8fB84s"
      },
      "source": [
        "## The Source of the Source\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtSvfDZAB84s"
      },
      "source": [
        "### Requesting a Selector\n",
        "Your task is to create a Selector object sel using the HTML source code stored in html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X0f8gkCtB84s",
        "outputId": "1bd88d3d-8b08-49a9-c400-8f46a7ed65ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1020 elements in the HTML document.\n",
            "You have found:  1020\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "url = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
        "\n",
        "# Create the string html containing the HTML source\n",
        "html = requests.get(url).content\n",
        "\n",
        "# Create the Selector object sel from html\n",
        "sel = Selector(text=html)\n",
        "\n",
        "# Print out the number of elements in the HTML document\n",
        "print(\"There are 1020 elements in the HTML document.\")\n",
        "print(\"You have found: \", len(sel.xpath('//*')))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}